{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Trade_data_code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TY5bYaQtt-3"
      },
      "source": [
        "# Demonstrate how to access Tick History using python with a service account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRfxqGL-tt-6"
      },
      "source": [
        "# This is the GoogleBigQuery client library to hide the REST API calls\n",
        "# Use e.g. 'pip install --upgrade google-cloud-bigquery' to add\n",
        "from google.cloud import bigquery\n",
        "# os is required to set the environment variable to point ot the JSON keyfile for authentication\n",
        "import os\n",
        "import pandas as pd"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a3MCVr3tt-6"
      },
      "source": [
        "# Point GOOGLE_APPLICATION_CREDENTIALS to the credentials key file\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'dbd-pscacc-sdlc-prod-e91cde0f4754.json'\n",
        "\n",
        "# Set the name of the GCP project where the queries are executed\n",
        "gcp_query_project=u'dbd-pscacc-sdlc-prod'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHlXpSLjtt-7"
      },
      "source": [
        "# Here is the SQL query: this instructs BigQuery to do the heavy lifting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dT2LGGXtt-7"
      },
      "source": [
        "# Define the SQL query\n",
        "query_expression = \"\"\"\n",
        "#standardSQL\n",
        "DECLARE start_time TIMESTAMP DEFAULT TIMESTAMP(\"2020-11-23 09:30:00.000000\");\n",
        "DECLARE end_time TIMESTAMP DEFAULT TIMESTAMP(\"2020-11-23 16:00:00.000000\");\n",
        "\n",
        "SELECT RIC as symbol,\n",
        "       Type,\n",
        "       -- aggregate, aggregate, aggregate to use the capabilities of the cloud service\n",
        "       Date_Time as date,\n",
        "       \"NSQ\" as exchange_id,\n",
        "       Price as price,\n",
        "       Volume as volume\n",
        "       \n",
        "-- this is the market data table (ASQ/BTQ/PCQ/NSQ/NMQ/NAQ/NYQ/PNK available)\n",
        "FROM `dbd-sdlc-prod.NSQ_NORMALISED.NSQ_NORMALISED`\n",
        "\n",
        "-- ALWAYS(!) use a Date_Time constraint, otherwise the entire table will be scanned (e.g. NYQ > 75 TerraByte)\n",
        "WHERE Type = \"Trade\" AND RIC = \"AAPL.O\" AND (Date_Time BETWEEN start_time AND end_time)\n",
        "\n",
        "-- don't filter too much: it's more efficent to use e.g. RIC IN (list) or pattern matching than a single instrument\n",
        "-- AND RIC LIKE '%.K'\n",
        "\n",
        "ORDER BY Date_Time\n",
        "\"\"\""
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5dvNB93tt-8"
      },
      "source": [
        "# Initialize a BigQuery client\n",
        "bq_client_object = bigquery.Client(project = gcp_query_project)\n",
        "\n",
        "# Initialize a BigQuery job config object to set various parameters\n",
        "job_config_object = bigquery.QueryJobConfig()\n",
        "\n",
        "# Dry run just gives back sizes and does not run the job!\n",
        "# Alway run it before executing the query to make sure the data scan is what you'd expect\n",
        "job_config_object.dry_run = True"
      ],
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uyza2C9tt-8",
        "outputId": "91c7c8ef-7200-47ff-9622-76a400ebecb2"
      },
      "source": [
        "# Initialize a BiqQuery job onject and pass it the config parameters\n",
        "# Run to see results here\n",
        "query_job_object = bq_client_object.query(query_expression, job_config = job_config_object)\n",
        "\n",
        "if job_config_object.dry_run:\n",
        "    print('Dry run - query will process {:,d} MB'.format(int(round(query_job_object.total_bytes_processed/1024**2))))\n",
        "else:\n",
        "    # This executes the query and puts it in a result iterator\n",
        "    query_result = query_job_object.result()\n",
        "    #print(type(query_result))\n",
        "    \n",
        "    # iterate through the result\n",
        "    for row in query_result:\n",
        "      print(row.date, row.Type, row.exchange_id, row.symbol, row.price, row.volume)"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dry run - query will process 1,853 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gb91zcERNK2"
      },
      "source": [
        "# Initialize a BiqQuery job onject and pass it the config parameters\n",
        "# Run to download results into a .csv file\n",
        "query_job_object = bq_client_object.query(query_expression, job_config = job_config_object)\n",
        "\n",
        "if job_config_object.dry_run:\n",
        "    print('Dry run - query will process {:,d} MB'.format(int(round(query_job_object.total_bytes_processed/1024**2))))\n",
        "else:\n",
        "    # This executes the query and puts it in a result iterator\n",
        "    query_result = query_job_object.result()\n",
        "    #print(type(query_result))\n",
        "    \n",
        "    # iterate through the result\n",
        "    empty_list = []\n",
        "    for row in query_result:\n",
        "      empty_list.append([row.date, row.Type, row.exchange_id, row.symbol, row.bid, row.bid_size, row.ask, row.ask_size])\n",
        "    df = pd.DataFrame(empty_list)\n",
        "    df.to_csv(r'C:\\Users\\robin\\Desktop\\quote_file.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}